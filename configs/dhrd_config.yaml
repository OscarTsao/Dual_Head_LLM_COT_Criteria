# Dual-Head Reasoning Distillation (DHRD) Configuration
# Based on paper: arXiv:2509.21487

# Model configuration
model:
  name_or_path: "gpt2"  # Can be changed to other decoder models (e.g., "meta-llama/Llama-2-7b-hf")
  num_labels: 2  # Number of classification labels
  dropout_prob: 0.1
  loss_weight_alpha: 0.5  # Weight for classification loss (1-alpha for LM loss)

# Data configuration
data:
  train_file: "data/train.json"
  eval_file: "data/eval.json"
  test_file: "data/test.json"
  max_input_length: 512
  max_rationale_length: 1024
  batch_size: 8
  num_workers: 4

# Training configuration
training:
  num_epochs: 3
  learning_rate: 5.0e-5
  weight_decay: 0.01
  warmup_steps: 100
  max_grad_norm: 1.0
  log_interval: 10
  eval_interval: 100
  save_dir: "./outputs/dhrd_experiment"

  # Optimizer settings
  optimizer:
    type: "adamw"
    betas: [0.9, 0.999]
    eps: 1.0e-8

  # Scheduler settings
  scheduler:
    type: "linear"  # Options: linear, cosine, constant
    warmup_ratio: 0.1

# Evaluation configuration
evaluation:
  batch_size: 16
  compute_throughput: true
  benchmark_runs: 3

# Hardware configuration
hardware:
  device: "cuda"  # Options: cuda, cpu
  mixed_precision: false
  gradient_accumulation_steps: 1

# SuperGLUE task configuration (optional)
superglue:
  task_name: "boolq"  # Options: boolq, cb, copa, multirc, rte, wic, wsc
  use_superglue: false

# Logging and experiment tracking
logging:
  use_mlflow: true
  experiment_name: "dhrd_experiments"
  tracking_uri: "file:./mlruns"
  log_model: true

# Reproducibility
seed: 42
